{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 3 - Comparación de distintos algoritmos de clasificación\n",
    "\n",
    "### Grupo M:\n",
    "     - Felipe Chavat - 4.659.492-2\n",
    "     - Leonel Rosano - 5.039.791-0\n",
    "     - Emiliano Pérez - 4.787.149-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este laboratorio es clasificar los dos conjuntos de datos _Iris_ y _Forest Covertypes_ utilizando los algoritmos _Naive Bayes_ y _K-NN_ (con K igual a 1, 3 y 7). Se evaluan los resultados de cada algoritmo y se comparan con el fin de obtener el mejor algoritmo para cada conjunto de datos. Para esta comparación se toman en cuenta también los resultados obtenidos del laboratorio anterior haciendo uso del algoritmo _ID3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Bayes Ingenuo\n",
    "Lleva el nombre de _Naive Bayes_ en ingles.\n",
    "Para poder generar un modelo clasificador de los conjuntos _Iris_ y _Cover Type_ con el algoritmo de _Bayes Ingenuo_ fue necesario implementar este con una para permitir atributos numéricos continuos (ya que principalmente este algoritmo fue diseñado para trabajar con atributos discretos). Las dos alternativas encontradas para poder trabajar con conjuntos que contienen atributos numéricos fueron\n",
    "- Discretizar los atributos, utilizando umbrales para subdividir el atributos en intervalos.\n",
    "- Utilizar funciones de densidad de probabilidad.\n",
    "\n",
    "En particular, nuestra solución utiliza la segunda alternativa, generando para cada atributo, una función de densidad de probabilidad normal (Gaussiana) por cada clase existente con la media y desviación estándar.\n",
    "\n",
    "Por ejemplo, dado un conjunto de datos de la forma:\n",
    "\n",
    "| Instance | Attr 0 | Attr 1 | Class |\n",
    "|---------:|-------:|-------:|------:|\n",
    "|1|10|2.5|1|\n",
    "|2|5.5|1|1|\n",
    "|3|1|10|0|\n",
    "\n",
    "Obtenemos las funciones de densidad de probabilidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Attributes distributions:\n",
      "- Attribute  0\n",
      "-- Class  0\n",
      "--- Mean:  1.0\n",
      "--- Std:  0.0\n",
      "-- Class  1\n",
      "--- Mean:  7.75\n",
      "--- Std:  2.25\n",
      "- Attribute  1\n",
      "-- Class  0\n",
      "--- Mean:  10.0\n",
      "--- Std:  0.0\n",
      "-- Class  1\n",
      "--- Mean:  1.75\n",
      "--- Std:  0.75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from naive import Naive\n",
    "data = np.array([[10,2.5,1],[5.5,1,1],[1,10,0]]).astype(float)\n",
    "Naive(data, [1,1]).showDists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones son luego utilizadas para calcular las probabilidades condicionadas.\n",
    "\n",
    "Este método es también llamado _Gaussian Naive Bayes_ haciendo referencia a la distribución Gaussiana.\n",
    "\n",
    "También fue necesario encontrar una solución al problema multiplicar computacionalmente un gran número de probabilidades probabilidades. \n",
    "Resolver computacionalmente el argumento máximo de la multiplicatoria de probabilidades necesaria para predecir usando el algoritmo de Bayes genera numeros demasiado pequeños, tan pequeños que no pueden ser representados como puntos flotantes (generan _nan_).\n",
    "La solución encontrada para este problema fue representar las probabilidades como probabilidades logaritmicas _[1]_. De esta forma, dado a que una probabilidad logarítmica se encontrará en el rango de (-inf, 0] y no [0, 1] los productos generados corren menos riesgo de caer en esta problemática.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN\n",
    "\n",
    "Para el algoritmo de _K-NN_ no fue necesario implementar un método para trabajar con atributos numéricos continuos, ya que éste soporta este tipo de atributo de forma natural.\n",
    "\n",
    "El algoritmo implementado utiliza la distancia euclidiana de norma 2 entre la instancia a predecir y cada instancia del conjunto de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algoritmos aplicados en conjunto _Iris_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Preparación del conjunto\n",
    "\n",
    "El conjunto Iris consta de 150 instancias en total, las cuales clasifican a la planta Iris en 3 distintas especies: Iris Setosa, Iris Versicolour, Iris Virginica. Los atributos utilizados para clasificar la especie son 4, todos numéricos continuos.\n",
    "Dado el conjunto de datos Iris, se reorganizan las tuplas de forma aleatoria para lograr una buena distribución de las clases a clasificar. Luego se particiona este conjunto en dos subconjuntos A y B correspondientes al 60% y 40% del conjunto original.\n",
    "\n",
    "El subconjunto A es utilizado para entrenar el algoritmo de _Bayes_ y cómo comparador de distancias en el algoritmo _K-NN_. El subconjunto B se utiliza para hacer una evaluación del modelo resultante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaluación de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Algoritmo de Bayes Ingenuo\n",
    "\n",
    "La evaluación realizada haciendo uso del modelo obtenido con el algoritmo de _Bayes Ingenuo_ da como resultados:\n",
    "\n",
    "|- |Prec |Rec |Fs |\n",
    "|---: |---: |---: |---:|\n",
    "|Micro |0.911 |0.911 |0.911|\n",
    "|Macro |0.937 |0.867 |0.9|\n",
    "\n",
    "Y la matriz de confusión resultante es:\n",
    "\n",
    "|-|Iris Setosa  |Iris Versicolour  |Iris Virginica   |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa  |**17** |0 |0 |\n",
    "| Iris Versicolour  |4 |**6** |0 |\n",
    "| Iris Virginica  |0 |0 |**18** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Algoritmo K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La evaluación de este algoritmo se realiza para valores de _k_ igual a 1, 3 y 7.\n",
    "Los resultados obtenidos son:\n",
    "\n",
    "#### Precisión:\n",
    "\n",
    "| K | Prec. Micro | Prec. Macro | Rec. Micro | Rec. Macro | Fs(0.5) Micro | Fs(0.5) Macro |\n",
    "|---: |---: |---: |---:|---: |---: |---: |\n",
    "| 1 | 0.911 | 0.937 | 0.911 | 0.867 | 0.911 | 0.9 |\n",
    "| 3 | 0.911 | 0.937 | 0.911 | 0.867 | 0.911 | 0.9 |\n",
    "| 7 | **1.0** | **1.0** | **1.0** | **1.0** | **1.0** | **1.0** |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Las matrices de confusión resultantes fueron:\n",
    "##### k = 1\n",
    "\n",
    "|-|Iris Setosa\t  |Iris Versicolour  |Iris Virginica\t  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa\t  |**17** |0 |0 |\n",
    "| Iris Versicolour  |4 |**6** |0 |\n",
    "| Iris Virginica\t  |0 |0 |**18** |\n",
    "\n",
    "##### k = 3\n",
    "\n",
    "|-|Iris Setosa\t  |Iris Versicolour  |Iris Virginica\t  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa\t  |**17** |0 |0 |\n",
    "| Iris Versicolour  |4 |**6** |0 |\n",
    "| Iris Virginica\t  |0 |0 |**18** |\n",
    "\n",
    "##### k = 7\n",
    "\n",
    "|-|Iris Setosa\t  |Iris Versicolour  |Iris Virginica\t  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa\t  |**17** |0 |0 |\n",
    "| Iris Versicolour  |0 |**10** |0 |\n",
    "| Iris Virginica\t  |0 |0 |**18** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Algoritmo ID3\n",
    "\n",
    "Haciendo uso de los resultados obtenidos del laboratorio anterior. Para el conjunto _Iris_, se lograron los mejores resultados generando un árbol de decisión con la métrica de _Misclassification_.\n",
    "Sin embargo, consideramos razonable mostrar cual es el resultado obtenido haciendo uso del mismo conjunto de prueba que para los algoritmos anteriores.\n",
    "\n",
    "Una de las variaciones que impemlentamos en el algoritmo fue la repetición de atributos, característica que nos permitió conseguir los mejores resultados para el conjunto de datos _Fores Cover Type_. Consideramos que es conveniente evaluar el algoritmo también con esta variación.\n",
    "\n",
    "Resultados:\n",
    "\n",
    "| Metric | Atr. Rep. | Prec. Micro | Prec. Macro | Rec. Micro | Rec. Macro | Fs(0.5) Micro | Fs(0.5) Macro |\n",
    "|-------:|--------------:|------------:|------------:|-----------:|-----------:|--------------:|-------:|\n",
    "| Entropy | No | 0.889 |  0.924 |0.889 | 0.833 | 0.889 | 0.876|\n",
    "| Gini | No | 0.889 |  0.924 |0.889 | 0.833 | 0.889 | 0.876|\n",
    "| Misclass | No | 0.889 |  0.924 |0.889 | 0.833 | 0.889 | 0.876|\n",
    "| Entropy | Yes | **0.911** | **0.937** | **0.911** |  **0.867**  | **0.911** |  **0.9**|\n",
    "\n",
    "\n",
    "La matriz de confusión obtenida usando _Entropía_, _GINI_ y _Misclassification_ fue:\n",
    "\n",
    "|-|Iris Setosa  |Iris Versicolour  |Iris Virginica  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa  |**17** |0 |0 |\n",
    "| Iris Versicolour  |5 |**5** |0 |\n",
    "| Iris Virginica  |0 |0 |**18** |\n",
    "\n",
    "\n",
    "Y la matriz de confusión obtenida haciendo uso de repetición de atributos fue:\n",
    "\n",
    "|-|Iris Setosa  |Iris Versicolour  |Iris Virginica  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa  |**17** |0 |0 |\n",
    "| Iris Versicolour  |4 |**6** |0 |\n",
    "| Iris Virginica  |0 |0 |**18** |\n",
    "\n",
    "Observación:\n",
    "Para la evaluación con repetición de atributos el resultado obtenido fue igual para todo umbral de repetición de atributo entre 0 y ~0.21. Para umbrales mas grandes, el resultado empeora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Conclusión\n",
    "\n",
    "Observando los datos obtenidos podemos ver que en el conjunto de datos _Iris_, los algoritmos de _Bayes Ingenuo_ y _K-NN_ con valor de K igual a 1 y 3 se comportaron de forma similar, obteniendo una precisión total de 91.11%.  Mientras _K-NN_ con valor de K igual a 7 se comportó de forma óptima, consiguiendo un 100% de precisión total. Se puede observar que al aumentar la cantidad de vecinos con los cuales hallar el promedio, se disminuye el error. Creemos que la precisión aumenta debido a la relación de los atributos en el conjunto de datos _Iris_, ya que al aumentar el valor de K, es mayor la cantidad de ejemplos similares a consultar, esto genera que sea más tolerante al ruido.\n",
    "\n",
    "Comparando los resultados obtenidos con el algoritmo _ID3_ del laboratorio 2 pudimos observar que si se implementa con la variante de “repetir los atributos”, este se comporta similar a Bayes y KNN con valores de K iguales a 1 y 3, dando una precisión de 91,11%. \n",
    "Mientras que si no se permite la repetición de atributos, la precisión desciende a 88,88%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Algoritmos aplicados en conjunto _Forest CoverType_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Preparación del conjunto\n",
    "\n",
    "El conjunto _Forest Covertype_ consta de 581.012 instancias de 10 atributos cuantitativos, 2 atributos cualitativos (de tipo one-hot representados con 4 y 40 bits), y 7 clases distintas.\n",
    "Se procedió a subdividir el conjunto en 3 subconjuntos. El primero correspondiente al 60% del conjunto original, el segundo y tercero al 20%. El primero se utiliza para entrenar, el segundo se para realizar evaluaciones y el tercero en caso de realizar otras pruebas (de esta forma aseguramos nos aseguramos que todos los modelos obtenidos se evalúen con el mismo conjunto de datos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Evaluación de los algoritmos\n",
    "\n",
    "### 4.2.1 Algoritmo de _Bayes Ingenuo_\n",
    "El algoritmo de _Bayes Ingenuo_ ejecutado sobre el conjunto de _Forest Covertype_ no obtuvo buenos resultados.\n",
    "Mas adelante en el informe se analisan cuales pueden haber sido los motivos por los cuales este algoritmo no es eficiente sobre el conjunto.\n",
    "\n",
    "Los resultados obtenidos fueron:\n",
    "\n",
    "| Set Size | Method | Prec. Micro | Prec. Macro | Rec. Micro | Rec. Macro | Fs(0.5) Micro | Fs(0.5) Macro |\n",
    "|-------:|------:|--------------:|------------:|------------:|-----------:|-----------:|--------------:|\n",
    "|5k|Normal|**0.569**|**0.454**|**0.569**|0.629|**0.569**|**0.527**|\n",
    "|50k|Normal|0.544|0.426|0.544|0.655|0.544|0.516|\n",
    "|50k|5FCV1|0.545|0.423|0.545|0.645|0.545|0.511|\n",
    "|50k|5FCV2|0.547|0.426|0.547|**0.665**|0.547|0.52|\n",
    "|50k|5FCV3|0.55|0.427|0.55|**0.665**|0.55|0.52|\n",
    "|50k|5FCV4|0.546|0.433|0.546|0.661|0.546|0.523|\n",
    "|50k|5FCV5|0.535|0.417|0.535|0.653|0.535|0.509|\n",
    "|500k|Normal|0.54|0.43|0.54|0.661|0.54|0.521|\n",
    "|500k|5FCV1|0.539|0.428|0.539|0.659|0.539|0.519|\n",
    "|500k|5FCV2|0.539|0.431|0.539|0.661|0.539|0.522|\n",
    "|500k|5FCV3|0.536|0.431|0.536|0.663|0.536|0.523|\n",
    "|500k|5FCV4|0.54|0.433|0.54|0.659|0.54|0.523|\n",
    "|500k|5FCV5|0.54|0.431|0.54|0.663|0.54|0.523|\n",
    "\n",
    "  \n",
    "**Observación**: en la tabla, _iFCVj_ hace referencia a i-Fold Cross Validation #j.\n",
    "\n",
    "Confussion Matrix:\n",
    "\n",
    "|-|1  |2  |3  |4  |5  |6  |7  | Accuracy |\n",
    "|---:|---:|---:|---:|---:|---:|---:|---:|----:|\n",
    "| **5K** |\n",
    "| 1  |**27133** |6521 |58 |0 |2224 |170 |6324 |**63.95%**|\n",
    "| 2  |14707 |**27639** |873 |1 |10186 |1953 |1212 |**48.86%**|\n",
    "| 3  |0 |6 |**4264** |460 |1018 |1471 |0 |**59.07%**|\n",
    "| 4  |0 |0 |133 |**302** |0 |105 |0 |55.93%|\n",
    "| 5  |45 |338 |9 |0 |**1401** |90 |0 |74.4%|\n",
    "| 6  |0 |42 |1240 |85 |339 |**1749** |0 |50.62%|\n",
    "| 7  |475 |10 |0 |0 |16 |0 |**3603** |**87.79%**|\n",
    "| **50K** |\n",
    "| 1  |**26537** |6449 |20 |0 |2426 |328 |6670 |62.54%|\n",
    "| 2  |14983 |**25961** |824 |27 |10789 |2381 |1606 |45.89%|\n",
    "| 3  |0 |10 |**3369** |1411 |994 |1435 |0 |46.67%|\n",
    "| 4  |0 |0 |34 |**469** |0 |37 |0 |86.85%|\n",
    "| 5  |70 |286 |20 |0 |**1401** |106 |0 |74.4%|\n",
    "| 6  |0 |38 |807 |434 |321 |**1855** |0 |53.69%|\n",
    "| 7  |446 |4 |0 |0 |18 |0 |**3636** |88.6%|\n",
    "| **500K**|\n",
    "| 1  |**26671** |6345 |14 |0 |2571 |290 |6539 |62.86%|\n",
    "| 2  |15099 |**25404** |768 |21 |11390 |2234 |1655 |44.91%|\n",
    "| 3  |0 |8 |**3184** |1359 |958 |1710 |0 |44.11%|\n",
    "| 4  |0 |0 |30 |**482** |0 |28 |0 |**89.26%**|\n",
    "| 5  |57 |265 |27 |0 |**1444** |90 |0 |**76.69%**|\n",
    "| 6  |0 |44 |693 |409 |350 |**1959** |0 |**56.7%**|\n",
    "| 7  |459 |5 |0 |0 |19 |0 |**3621** |88.23%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Algoritmo de _K-NN_\n",
    "El algoritmo _K-NN_ se ejecutó haciendo uso de un conjunto de entrenamiento de 5.000 (5k), 50.000 (50k) y 361.000 (500k) instancias. Esto con el objetivo (al igual que con los otros algoritmos) de observar cual es la \"mejoría\" que genera entrenar (en este caso no sería entrenar sinó comparar distancias) con mas instancias.\n",
    "Ademas de esto, el algoritmo también se ejecuta para distintos valores de _K_, en especifico para K igual a 1, 3, 5 y 7. Se toman valores de K impares, ya que el resultado de la predicción quedara determinada por la clase mas repetida entre los k vecinos mas cercanos, de esta forma se mitigan los empates.\n",
    "\n",
    "Los resultados obtenidos fueron los siguientes:\n",
    "\n",
    "| Set Size | K | Prec. Micro | Prec. Macro | Rec. Micro | Rec. Macro | Fs(0.5) Micro | Fs(0.5) Macro |\n",
    "|-------:|------:|--------------:|------------:|------------:|-----------:|-----------:|--------------:|\n",
    "| 5K | 1 |0.733|0.733| 0.733|0.573|0.733|0.585|\n",
    "| 5K | 3 |0.715|0.621|0.715|0.478|0.715|0.54|\n",
    "| 5K | 5 |0.71|0.596|0.71|0.426|0.71|0.497|\n",
    "| 5K | 7 |0.705|0.573|0.705|0.394|0.705|0.466|\n",
    "| 50K | 1 |0.903|0.842|0.903|0.832|0.903|0.837|\n",
    "| 50K | 3 |0.883|0.842|0.883|0.783|0.883|0.812|\n",
    "| 50K | 5 |0.869|0.83|0.869|0.748|0.869|0.787|\n",
    "| 50K | 7 |0.859|0.824|0.859|0.723|0.859|0.77|\n",
    "| 500K | 1 |0.962|0.936|0.962|0.929|0.962|0.932|\n",
    "| 500K | 3 |**0.964**|**0.944**|**0.964**|**0.922**|**0.964**|**0.933**|\n",
    "| 500K | 5 |0.961|0.941|0.961|0.912|0.961|0.926|\n",
    "| 500K | 7 |0.957|0.94|0.957|0.904|0.957|0.922|\n",
    "\n",
    "La matriz de confusión obtenida para el mejor caso (Tamaño 500k y K=3):\n",
    "\n",
    "|-|1  |2  |3  |4  |5  |6  |7  |Accuracy|\n",
    "|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n",
    "| 1  |**40883** |1430 |1 |0 |26 |1 |89 |96.35%|\n",
    "| 2  |1321 |**54965** |79 |0 |139 |57 |10 |97.16%|\n",
    "| 3  |2 |104 |**6948** |34 |10 |121 |0 |96.25%|\n",
    "| 4  |0 |1 |81 |**426** |0 |32 |0 |78.89%|\n",
    "| 5  |32 |179 |10 |0 |**1659** |3 |0 |88.1%|\n",
    "| 6  |4 |93 |146 |12 |3 |**3197** |0 |92.53%|\n",
    "| 7  |131 |24 |0 |0 |3 |0 |**3946** |96.15%|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Algoritmo ID3\n",
    "\n",
    "Del laboratorio anterior obtuvimos el mejor resultado al aplicar el algorimto _ID3_ permitiendo repetir atributos, es decir, no descartando el atributo una vez que este es elegido como el mejor atributo, sino que se descarta una vez que la ganancia que se obtiene de obtener este atributo es menor a un umbral definido.\n",
    "En particular, los mejores resultados se obtuvieron con un umbral igual a 0.001.\n",
    "En la siguiente tabla se muestra el mejor resultado para cada una de las distintas variantes realizadas.\n",
    "\n",
    "\n",
    "|THRESH|Metric| Method | #Tuples to train |Prec Micro|Prec Macro|Rec Micro|Rec Macro| Micro F(0.5) | Macro F(0.5)| \n",
    "|---:|-----:|-------:|-----------------:|-------------:|-------------:|--:|--:|--:|--:|\n",
    "|NO|Entropy|Binary Forest|500k|0.736|0.7|0.736|0.521|0.736|0.597|\n",
    "|0.1|Entropy|Normal|50k|0.692|0.766|0.692|0.395|0.692|0.521|\n",
    "|0.01|Entropy|Normal|500k|0.864|0.791|0.864|0.766|0.864|0.778|\n",
    "|0.001|Entropy|Normal|500k|**0.879**|**0.818**|**0.879**|**0.792**|**0.879**|**0.805**|\n",
    "\n",
    "\n",
    "Y la matriz de confusión obtenida para el mejor caso fue:\n",
    "\n",
    "|Class|1  |2  |3  |4  |5  |6  |7  |Accuracy|\n",
    "|---:|---:|---:|---:|---:|---:|---:|---:|----:|\n",
    "| 1  |**36252** |5722 |14 |0 |100 |16 |410 |85.27%|\n",
    "| 2  |4994 |**50317** |353 |1 |500 |226 |83 |89.1%|\n",
    "| 3  |6 |324 |**6221** |95 |29 |565 |0 |85.82%|\n",
    "| 4  |0 |8 |181 |**263** |0 |84 |0 |49.07%|\n",
    "| 5  |74 |522 |35 |0 |**1269** |10 |0 |66.44%|\n",
    "| 6  |15 |317 |636 |26 |22 |**2442** |0 |70.62%|\n",
    "| 7  |357 |69 |0 |0 |0 |0 |**3644** |89.53%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Comparación de algoritmos y Conclusión\n",
    "\n",
    "Con un dataset de mayor magnitud como lo es _Covertype_, pudimos observar que los porcentajes varían significativamente, sobre todo en el algoritmo de _Bayes Ingenuo_ el cual nos da una precisión de 56,9% con un conjunto de entrenamiento de 5000 instancias. Si aumentamos el conjunto de entrenamiento a 50.000 y 348.607 la precisión disminuye aún más, llegando a 54,7% y 54% respectivamente. \n",
    "\n",
    "El algoritmo de _K-NN_ se comporta de forma similar que con el conjunto _Iris_.\n",
    "Se registró un aumento del porcentaje al aumentar el _k_, lo mismo ocurre al aumentar el conjunto de entrenamiento.\n",
    "\n",
    "\n",
    "### 4.3.1. ¿Por qué _Bayes Ingenuo_ no dió buenos resultados?\n",
    "\n",
    "Creemos que el algoritmo de _Bayes Ingenuo_ no dió buenos resultados dado que éste toma como hipótesis que cada atributo se comporta de forma independiente, lo cual realizando un estudio de cuán relacionados están los atributos. Dado que la correlación entre dos atributos implica dependencia entre estos [2] creemos que esta es una de las razónes por las cuales un algoritmo que hace una suposición tan fuerte como la independencia entre atributos puede verse degradado.\n",
    "\n",
    "Generamos gráficas de correlación entre los primeros 10 atributos, aquellos cuyos valores son numéricos continuos:\n",
    "\n",
    "![](images/correlation1.png)\n",
    "![](images/correlation2.png)\n",
    "\n",
    "Se observó que los atributos con mayor correlación son:\n",
    "Hillshade_3pm, Hillshade_9pm: -0.78\n",
    "Hillshade_3pm, Hillshade_Noon: 0.594\n",
    "Hillshade_9am, Aspect: -0.579\n",
    "Hillshade_3pm, Aspect: 0.647\n",
    "Hillshade_Noon, Slope: -0.528\n",
    "Vertical_Distance_To_Hidrology, Horizontal_Distance_To_Hydrology: 0.606\n",
    "\n",
    "\n",
    "También creemos posible que el hecho de haber generado una función de distribución Gaussiana de probabilidad para cada atributo continuo puede ser una suposición también muy fuerte ya que es posible que algunos atributos no sigan una distribución Gaussiana.\n",
    "\n",
    "\n",
    "### 4.3.2. Performance en entrenamiento y clasificación\n",
    "\n",
    "Consideramos conveniente hacer un análisis en el tiempo que lleva entrenar y clasificar cada uno de los algoritmo implementados en función de la cantidad de tuplas que contiene el conjunto de entrenamiento.\n",
    "Los resultados obtenidos fueron:\n",
    "\n",
    "**Observación**: Eje X de las gráficas 1 equivale a 1000 tuplas en conjunto de entrenamiento.\n",
    "![](images/performance1.png)\n",
    "![](images/performance2.png)\n",
    "\n",
    "\n",
    "Para generar un modelo de predicción con los algoritmos _ID3_ y _Bayes Ingenuo_ es necesario \"entrenar\" haciendo uso del conjunto de entrenamiento. Por esta razón, el algorítmo consumirá tiempo en esta etapa.\n",
    "Puede observarse en las gráficas que a medida que el conjunto de entrenamiento aumenta el tiempo necesario para generar un modelo aumenta también. Sin embargo, el tiempo que lleva predecir una instancia es muy chico, de hecho, en un modelo generado por _ID3_ el tiempo que lleva la clasificación es muy cercano a 0 (menor a 0.1e-17).\n",
    "El algoritmo _K-NN_ no requiere de entreniamiento previo, sin embargo, el tiempo necesario para clasificar una instancia aumenta a medida que el conjunto de entrenamiento aumenta.\n",
    "\n",
    "### 4.3.3 Conclusión\n",
    "\n",
    "Comparando los algoritmos, pudimos concluir que para el conjunto _covertype_ el algoritmo que mejores resultados arroja es _K-NN_ ya que el mejor resultado obtenido para ID3 fue de 86%. Pero en comparación con _Bayes Ingenuo_, _ID3_ tiene una mejor precisión.\n",
    "\n",
    "En cuanto a tiempo de performance, podríamos decir que _K-NN_ requiere mas tiempo para clasificar una instancia que _ID3_ y _Bayes Ingenuo_. Podría existir el caso en el que sería inviable usar un modelo generado por el algoritmo _K-NN_ por el tiempo que a este le lleva estimar. En este caso, el modelo generado con _ID3_ sería la mejor opción (de los tres algoritmos) dado que clasifica en tiempo muy cercano a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Manual de usuario\n",
    "\n",
    "El paquete de archivos para realizar este experimento se encuentra en la carpeta `aprendAut`. Esta contiene los módulos necesarios para realizar las tareas y poder replicar los resultados obtenidos.\n",
    "\n",
    "Para preparar los conjuntos de entrenamiento y de evaluación del conjunto de datos _covertype_ se debe crear la carpeta `cov_data` al nivel de la carpeta `iris_data`. \n",
    "Dentro de `cov_data` se debe colocar el conjunto de datos `covertype.data` y ejecutar el módulo `dataPartitioner.py`. \n",
    "Una vez creadas las particiones, se débe correr `main.py`. Se expondrá un menú con las opciones para ejecutar _Bayes Ingenuo_ y _K-NN_ con los conjuntos _Iris_ y _CoverType_.\n",
    "\n",
    " \n",
    "También se encuentran en el módulo `main.py` funciones para poder realizar el estudio de los datos y el k-fold cross-validation de los algoritmos.\n",
    "Para correrlas, se debe comentar el menu y escribir el nombre de la función deseada al final del código.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "[1] - Rhodes College. Phillip Kirlin. _Using log-probabilities for Naive Bayes_.\n",
    "http://www.cs.rhodes.edu/~kirlinp/courses/ai/f18/projects/proj3/naive-bayes-log-probs.pdf\n",
    "\n",
    "[2] - Stack Overflow. _Does non-zero correlation imply dependence?_. https://stats.stackexchange.com/questions/113417/does-non-zero-correlation-imply-dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

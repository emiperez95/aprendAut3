{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 3 - Comparación de distintos algoritmos de clasificación\n",
    "\n",
    "### Grupo M:\n",
    "     - Felipe Chavat - 4.659.492-2\n",
    "     - Leonel Rosano - 5.039.791-0\n",
    "     - Emiliano Pérez - 4.787.149-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este laboratorio es clasificar los dos conjuntos de datos _Iris_ y _Forest Covertypes_ utilizando los algoritmos _Naive Bayes_ y _K-NN_ (con K igual a 1, 3 y 7). Se evaluan los resultados de cada algoritmo y se comparan con el fin de obtener el mejor algoritmo para cada conjunto de datos. Para esta comparación se toman en cuenta también los resultados obtenidos del laboratorio anterior haciendo uso del algoritmo _ID3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Ingenuo\n",
    "Lleva el nombre de _Naive Bayes_ en ingles.\n",
    "Para poder generar un modelo clasificador de los conjuntos _Iris_ y _Cover Type_ con el algoritmo de _Bayes Ingenuo_ fue necesario implementar este con una para permitir atributos numéricos continuos (ya que principalmente este algoritmo fue diseñado para trabajar con atributos discretos). Las dos alternativas encontradas para poder trabajar con conjuntos que contienen atributos numéricos fueron\n",
    "- Discretizar los atributos, utilizando umbrales para subdividir el atributos en intervalos.\n",
    "- Utilizar funciones de densidad de probabilidad.\n",
    "\n",
    "En particular, nuestra solución utiliza la segunda alternativa, generando para cada atributo, una función de densidad de probabilidad normal (Gaussiana) por cada clase existente con la media y desviación estándar.\n",
    "\n",
    "Por ejemplo, dado un conjunto de datos de la forma:\n",
    "\n",
    "| Instance | Attr 0 | Attr 1 | Class |\n",
    "|---------:|-------:|-------:|------:|\n",
    "|1|10|2.5|1|\n",
    "|2|5.5|1|1|\n",
    "|3|1|10|0|\n",
    "\n",
    "Obtenemos las funciones de densidad de probabilidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Attribute 0:\n",
      "-- Class 1.0:\n",
      "---  Mean:  7.75\n",
      "---  Std:  2.25\n",
      "-- Class 0.0:\n",
      "---  Mean:  1.0\n",
      "---  Std:  0.0\n",
      "- Attribute 1:\n",
      "-- Class 1.0:\n",
      "---  Mean:  1.75\n",
      "---  Std:  0.75\n",
      "-- Class 0.0:\n",
      "---  Mean:  10.0\n",
      "---  Std:  0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from naive import Naive\n",
    "data = np.array([[10,2.5,1],[5.5,1,1],[1,10,0]]).astype(float)\n",
    "Naive(data, [1,1]).showDists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones son luego utilizadas para calcular las probabilidades condicionadas.\n",
    "\n",
    "Este método es también llamado _Gaussian Naive Bayes_ haciendo referencia a la distribución Gaussiana.\n",
    "\n",
    "También fue necesario encontrar una solución al problema multiplicar computacionalmente un gran número de probabilidades probabilidades. \n",
    "Resolver computacionalmente el argumento máximo de la multiplicatoria de probabilidades necesaria para predecir usando el algoritmo de Bayes genera numeros demasiado pequeños, tan pequeños que no pueden ser representados como puntos flotantes (generan _nan_).\n",
    "La solución encontrada para este problema fue representar las probabilidades como probabilidades logaritmicas _[1]_. De esta forma, dado a que una probabilidad logarítmica se encontrará en el rango de (-inf, 0] y no [0, 1] los productos generados corren menos riesgo de caer en esta problemática.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algoritmos aplicados en conjunto _Iris_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Preparación del conjunto\n",
    "\n",
    "El conjunto Iris consta de 150 instancias en total, las cuales clasifican a la planta Iris en 3 distintas especies: Iris Setosa, Iris Versicolour, Iris Virginica. Los atributos utilizados para clasificar la especie son 4, todos numéricos continuos.\n",
    "Dado el conjunto de datos Iris, se reorganizan las tuplas de forma aleatoria para lograr una buena distribución de las clases a clasificar. Luego se particiona este conjunto en dos subconjuntos A y B correspondientes al 60% y 40% del conjunto original.\n",
    "\n",
    "El subconjunto A es utilizado para entrenar el algoritmo de _Bayes_ y cómo comparador de distancias en el algoritmo _K-NN_. El subconjunto B se utiliza para hacer una evaluación del modelo resultante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Evaluación de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Algoritmo de Bayes Ingenuo\n",
    "\n",
    "La evaluación realizada haciendo uso del modelo obtenido con el algoritmo de _Bayes Ingenuo_ da como resultados:\n",
    "\n",
    "|- |Prec |Rec |Fs |\n",
    "|---: |---: |---: |---:|\n",
    "|Micro |0.911 |0.911 |0.911|\n",
    "|Macro |0.937 |0.867 |0.9|\n",
    "\n",
    "Y la matriz de confusión resultante es:\n",
    "\n",
    "|-|Iris Setosa  |Iris Versicolour  |Iris Virginica   |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa  |**17** |0 |0 |\n",
    "| Iris Versicolour  |4 |**6** |0 |\n",
    "| Iris Virginica  |0 |0 |**18** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Algoritmo K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La evaluación de este algoritmo se realiza para valores de _k_ igual a 1, 3 y 7.\n",
    "Los resultados obtenidos son:\n",
    "\n",
    "#### Precisión:\n",
    "\n",
    "| K | Prec. Micro | Prec. Macro | Rec. Micro | Rec. Macro | Fs(0.5) Micro | Fs(0.5) Macro |\n",
    "|---: |---: |---: |---:|---: |---: |---: |\n",
    "| 1 | 0.911 | 0.937 | 0.911 | 0.867 | 0.911 | 0.9 |\n",
    "| 3 | 0.911 | 0.937 | 0.911 | 0.867 | 0.911 | 0.9 |\n",
    "| 7 | **1.0** | **1.0** | **1.0** | **1.0** | **1.0** | **1.0** |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Las matrices de confusión resultantes fueron:\n",
    "##### k = 1\n",
    "\n",
    "|-|Iris Setosa\t  |Iris Versicolour  |Iris Virginica\t  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa\t  |**17** |0 |0 |\n",
    "| Iris Versicolour  |4 |**6** |0 |\n",
    "| Iris Virginica\t  |0 |0 |**18** |\n",
    "\n",
    "##### k = 3\n",
    "\n",
    "|-|Iris Setosa\t  |Iris Versicolour  |Iris Virginica\t  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa\t  |**17** |0 |0 |\n",
    "| Iris Versicolour  |4 |**6** |0 |\n",
    "| Iris Virginica\t  |0 |0 |**18** |\n",
    "\n",
    "##### k = 7\n",
    "\n",
    "|-|Iris Setosa\t  |Iris Versicolour  |Iris Virginica\t  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa\t  |**17** |0 |0 |\n",
    "| Iris Versicolour  |0 |**10** |0 |\n",
    "| Iris Virginica\t  |0 |0 |**18** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Algoritmo ID3\n",
    "\n",
    "Haciendo uso de los resultados obtenidos del laboratorio anterior. Para el conjunto _Iris_, se lograron los mejores resultados generando un árbol de decisión con la métrica de _Misclassification_.\n",
    "Sin embargo, consideramos razonable mostrar cual es el resultado obtenido haciendo uso del mismo conjunto de prueba que para los algoritmos anteriores.\n",
    "\n",
    "Una de las variaciones que impemlentamos en el algoritmo fue la repetición de atributos, característica que nos permitió conseguir los mejores resultados para el conjunto de datos _Fores Cover Type_. Consideramos que es conveniente evaluar el algoritmo también con esta variación.\n",
    "\n",
    "Resultados:\n",
    "\n",
    "| Metric | Atr. Rep. | Prec. Micro | Prec. Macro | Rec. Micro | Rec. Macro | Fs(0.5) Micro | Fs(0.5) Macro |\n",
    "|-------:|--------------:|------------:|------------:|-----------:|-----------:|--------------:|-------:|\n",
    "| Entropy | No | 0.889 |  0.924 |0.889 | 0.833 | 0.889 | 0.876|\n",
    "| Gini | No | 0.889 |  0.924 |0.889 | 0.833 | 0.889 | 0.876|\n",
    "| Misclass | No | 0.889 |  0.924 |0.889 | 0.833 | 0.889 | 0.876|\n",
    "| Entropy | Yes | **0.911** | **0.937** | **0.911** |  **0.867**  | **0.911** |  **0.9**|\n",
    "\n",
    "\n",
    "La matriz de confusión obtenida usando _Entropía_, _GINI_ y _Misclassification_ fue:\n",
    "\n",
    "|-|Iris Setosa  |Iris Versicolour  |Iris Virginica  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa  |**17** |0 |0 |\n",
    "| Iris Versicolour  |5 |**5** |0 |\n",
    "| Iris Virginica  |0 |0 |**18** |\n",
    "\n",
    "\n",
    "Y la matriz de confusión obtenida haciendo uso de repetición de atributos fue:\n",
    "\n",
    "|-|Iris Setosa  |Iris Versicolour  |Iris Virginica  |\n",
    "|---:|---:|---:|---:|\n",
    "| Iris Setosa  |**17** |0 |0 |\n",
    "| Iris Versicolour  |4 |**6** |0 |\n",
    "| Iris Virginica  |0 |0 |**18** |\n",
    "\n",
    "Observación:\n",
    "Para la evaluación con repetición de atributos el resultado obtenido fue igual para todo umbral de repetición de atributo entre 0 y ~0.21. Para umbrales mas grandes, el resultado empeora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Conclusión\n",
    "\n",
    "Observando los datos obtenidos podemos ver que en el conjunto de datos _Iris_ los algoritmos de _Bayes Ingenuo_ y _K-NN_ con valor de K igual a 1 y 3 se comportaron de forma similar, obteniendo una precisión total de 91.11%.  Mientras _K-NN_ con valor de K igual a 7 se comportó de forma óptima, consiguiendo un 100% de precisión total. Se puede observar que al aumentar la cantidad de vecinos con los cuales hallar el promedio, se disminuye el error. Creemos que la precisión aumenta debido a la relación de los atributos en el conjunto de datos _Iris_, ya que al aumentar el valor de K, es mayor la cantidad de ejemplos similares a consultar, esto genera que sea más tolerante al ruido.\n",
    "\n",
    "Comparando los resultados obtenidos con el algoritmo _ID3_ del laboratorio 2 pudimos observar que si se implementa con la variante de “repetir los atributos”, este se comporta similar a Bayes y KNN con valores de K iguales a 1 y 3, dando una precisión de 91,11%. \n",
    "Mientras que si no se permite la repetición de atributos, la precisión desciende a 88,88%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "Nombre de la organización. (Año). Título del informe (Número de la publicación). Recuperado de http://xxx.xxxxxx.xxx/\n",
    "\n",
    "\n",
    "[1] - Rhodes College. Phillip Kirlin. _Using log-probabilities for Naive Bayes_.\n",
    "http://www.cs.rhodes.edu/~kirlinp/courses/ai/f18/projects/proj3/naive-bayes-log-probs.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
